{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How might mixing the baskets with different kinds of data affect compression of the data upon filling the tree and saving to file? Does this have an effect with the overall disk usage or CPU walltime?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixing different kinds of data could affect compression because different kinds of data have different patterns/statistical properties. That said, something like `zlib` would usually flush out the internal structures it uses to track those patterns every so often specifically to account for those patterns changing over time. So if you had a dataset that was, say, lines of text representing phone numbers and lines of text representing addresses, you would get your most ideal compression if you had all of the phone numbers come before all of the addresses (or vice versa), and the more the phone number and addresses are interleaved, the worse compression would become. \n",
    "\n",
    "It's very much like cache misses -- the compressor is looking for patterns its seen before, and it only has so much space to store those patterns, so ideally you'd want all the data associated with one set of patterns to be processed, and then all the data with another set of patterns, and so on.\n",
    "\n",
    "Having multiple types of data might make the compressed size of the data larger, which would increase disk usage.\n",
    "\n",
    "CPU walltime could also increase because the compressor has to do more work -- getting back to the cache misses analogy, it takes more time for the compressor to record a new pattern than to refer back to a pattern its already seen. If the a particular pattern keeps getting flushed out only to be encountered again later, then redundant work is being done.\n",
    "\n",
    "*This is all assuming a Lempel-Ziv based compressor, which `zlib` is, and of course that's the ROOT default, so...\n",
    "**Also this is really only looking at one step of what happens in a compression pipeline, but it is the actual compression part.\n",
    "***There's also the obvious observation about data types, which is that larger data types will take more time to process and (to make a huge generalization) probably won't compress as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "2. Plot some of the histograms by calling back to the original hvector tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
